{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor,\\\n",
    "                             RandomForestRegressor, AdaBoostRegressor\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "train_filename = \"train.csv\"\n",
    "#test_filename = \"test.csv\"\n",
    "\n",
    "def load_data(fname):\n",
    "    df = pd.DataFrame.from_csv(fname, index_col=None)    \n",
    "    return df    \n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', 65)  # default: 50\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contains_weird_digit(x):\n",
    "    for a in x:\n",
    "        if not (str.isalnum(a) or str.isspace(a) or a in '!?.,;-()\\'\":/\\\\$+=#@%&'):\n",
    "            return True\n",
    "    return False\n",
    "#train[train.question1.apply(contains_weird_digit)].shape\n",
    "#train.question1[:10].apply(lambda q: filter(lambda x:str.isalnum(x) or str.isspace(x), q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading challenge... done.\n"
     ]
    }
   ],
   "source": [
    "print 'Loading challenge...',\n",
    "train = load_data(train_filename)\n",
    "train = train.dropna()\n",
    "# test = load_data(test_filename)\n",
    "# test.iloc[np.where(test.isnull())]='why'\n",
    "print 'done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.question1 = train.question1.apply(lambda x: x.lower())\n",
    "train.question2 = train.question2.apply(lambda x: x.lower())\n",
    "# test.question1 = test.question1.apply(lambda x: x.lower())\n",
    "# test.question2 = test.question2.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\py2\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "D:\\Program Files\\Anaconda3\\envs\\py2\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train.loc[train.question1.apply(contains_weird_digit) , \"question1\"] = np.nan\n",
    "train.loc[train.question2.apply(contains_weird_digit) , \"question1\"] = np.nan\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.35 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "words_lst_iterator1 = itertools.chain(*[''.join(map(lambda x: x if str.isalnum(x) else ' ', q)).split() for q in train.question1])\n",
    "words_lst_iterator2 = itertools.chain(*[''.join(map(lambda x: x if str.isalnum(x) else ' ', q)).split() for q in train.question2])\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(itertools.chain(words_lst_iterator1, words_lst_iterator2))\n",
    "end = time()\n",
    "print '{:.2f} seconds'.format(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = sorted(c.keys(), key=lambda x:c[x], reverse=True)\n",
    "freq = [c[w] for w in words]\n",
    "\n",
    "# plt.plot(freq[:100])\n",
    "# plt.xlabel('word num')\n",
    "# plt.ylabel('num occurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def intersect(x, y):\n",
    "    if type(x) is not set:\n",
    "        x = set(x)\n",
    "    return set(filter(x.__contains__, y))\n",
    "\n",
    "def union(x,y):\n",
    "    return set(x).union(y)\n",
    "\n",
    "def word_weight(w):\n",
    "    return 1\n",
    "\n",
    "def word_weight2(w):\n",
    "    return 1./(c[w]+1)\n",
    "\n",
    "def word_weight3(w):\n",
    "    return 1./np.sqrt(c[w]+1)\n",
    "\n",
    "def word_weight4(w):\n",
    "    return 1./(c[w]+1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uncommon_words_set = set(words[100:])\n",
    "\n",
    "def similarity1(lst1, lst2, weight_fun = word_weight):\n",
    "    return (1 + sum(weight_fun(w) for w in intersect(lst1, lst2))) *1./ (1 + sum(weight_fun(w) for w in union(lst1, lst2)))\n",
    "\n",
    "def similarity2(lst1, lst2, weight_fun = word_weight):\n",
    "    return  (1 + sum(weight_fun(w) for w in intersect(uncommon_words_set, intersect(lst1, lst2)))) *1./\\\n",
    "            (1 + sum(weight_fun(w) for w in intersect(uncommon_words_set, union(lst1, lst2))))\n",
    "    \n",
    "def similarity3(lst1, lst2):\n",
    "    return similarity1(lst1, lst2, weight_fun=word_weight2)\n",
    "\n",
    "def similarity4(lst1, lst2):\n",
    "    return similarity1(lst1, lst2, weight_fun=word_weight3)\n",
    "\n",
    "def similarity5(lst1, lst2):\n",
    "    return similarity1(lst1, lst2, weight_fun=word_weight4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2 = train.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.77 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for n in [1,2]:\n",
    "    train2['question{}_words'.format(n)] = [''.join(map(lambda x: x if str.isalnum(x) else ' ', q)).split() for q in train2['question{}'.format(n)]]\n",
    "#     test['question{}_words'.format(n)] = [''.join(map(lambda x: x if str.isalnum(x) else ' ', q)).split() for q in test['question{}'.format(n)]]\n",
    "end = time()\n",
    "print '{:.2f} seconds'.format(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.11 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "sim_fun_lst = [similarity1, similarity2, similarity3, similarity4, similarity5]\n",
    "for sim_fun in sim_fun_lst:\n",
    "    train2[sim_fun.__name__] = [sim_fun(lst1, lst2) for lst1,lst2 in itertools.izip(train2.question1_words, train2.question2_words)]\n",
    "end = time()\n",
    "print '{:.2f} seconds'.format(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indicator1(x):\n",
    "    return 'how can you' in x or 'how can i' in x or 'how do i' in x or 'how do you' in x\n",
    "\n",
    "train2['question1_ind1'] = train2.question1.apply(indicator1)\n",
    "train2['question2_ind1'] = train2.question2.apply(indicator1)\n",
    "# train2['ind1_prod'] = train2['question1_ind1']*train2['question2_ind1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indicator2(x):\n",
    "    return 'india' in x\n",
    "# print train2.question1.apply(indicator2).mean()\n",
    "train2['question1_ind2'] = train2.question1.apply(indicator2)\n",
    "train2['question2_ind2'] = train2.question2.apply(indicator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [1,2]:\n",
    "    train2['len{}'.format(n)] = train2['question{}_words'.format(n)].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['similarity1', 'similarity2', 'similarity3', 'similarity4', 'similarity5', 'question1_ind1', 'question2_ind1', 'len1', 'len2']\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "columns += ['similarity{}'.format(k) for k in [1,2,3,4,5]]\n",
    "columns += ['question{}_ind{}'.format(n,i) for n in [1,2] for i in [1]]\n",
    "columns += ['len{}'.format(n) for n in [1,2]]\n",
    "# columns += ['question{}_{}'.format(n,q) for n in [1,2] for q in q_words]\n",
    "print columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = train2.shape[0]/2\n",
    "train_set = train2.iloc[:train_size]\n",
    "valid_set = train2.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(yhat, y):\n",
    "    return -np.mean(y*np.log(yhat) + (1-y)*np.log(1-yhat))\n",
    "def baseline_score(train_df, test_df):\n",
    "    p = train_df.is_duplicate.mean()\n",
    "    print 'p={}'.format(p)\n",
    "    return logloss(np.array([p]*test_df.shape[0]), test_df.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=0.375830053795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6585011171362957"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_score(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['similarity1', 'similarity2', 'similarity3', 'similarity4', 'similarity5', 'question1_ind1', 'question2_ind1', 'len1', 'len2']\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2806            1.72m\n",
      "         2           1.2448            1.05m\n",
      "         3           1.2139           48.69s\n",
      "         4           1.1884           40.87s\n",
      "         5           1.1657           35.94s\n",
      "         6           1.1465           32.64s\n",
      "         7           1.1301           30.15s\n",
      "         8           1.1156           28.33s\n",
      "         9           1.1031           26.78s\n",
      "        10           1.0921           25.49s\n",
      "        20           1.0283           18.70s\n",
      "        30           1.0057           14.31s\n",
      "        40           0.9955           10.16s\n",
      "        50           0.9890            6.51s\n",
      "        60           0.9848            3.18s\n",
      "        70           0.9813            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1000,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=70, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'columns: {}'.format(columns)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(verbose=1, n_estimators=70, learning_rate=0.1, max_depth=3, min_samples_leaf=1000)\n",
    "clf.fit(train_set[columns], train_set.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['similarity1', 'similarity2', 'similarity3', 'similarity4', 'similarity5', 'question1_ind1', 'question2_ind1', 'len1', 'len2']\n",
      "0.635343941035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\py2\\lib\\site-packages\\ipykernel\\__main__.py:2: RuntimeWarning: invalid value encountered in log\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "print 'columns: {}'.format(columns)\n",
    "yhat = clf.predict_proba(valid_set[columns])[:,1] + clf.predict_proba(valid_set[columns])[:,1]\n",
    "print logloss(yhat, valid_set.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity1         : 0.348207951847\n",
      "similarity2         : 0.228791734666\n",
      "similarity3         : 0.122591582212\n",
      "similarity4         : 0.0790651971588\n",
      "similarity5         : 0.0152460397491\n",
      "question1_ind1      : 0.00598714950562\n",
      "question2_ind1      : 0.0208234836714\n",
      "len1                : 0.0965417834897\n",
      "len2                : 0.0827450777001\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(columns, clf.feature_importances_):\n",
    "    print '{:20}: {}'.format(name, score)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
